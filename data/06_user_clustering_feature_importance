# === æœ€ç»ˆå†²åˆºï¼šç”¨æˆ·èšç±» + å¯è§£é‡Šæ€§åˆ†æ ===
def final_sprint_analysis():
    """
    æœ€ç»ˆå†²åˆºåˆ†æï¼š
    1. ç”¨æˆ·æƒ…ç»ªè½¨è¿¹èšç±»
    2. ç®€åŒ–ç‰ˆSHAPè§£é‡Š
    3. å¯è§†åŒ–å‡†å¤‡
    """
    
    print("=== æœ€ç»ˆå†²åˆºåˆ†æ ===")
    
    try:
        # é˜¶æ®µ1ï¼šç”¨æˆ·èšç±»åˆ†æ
        print("1. ç”¨æˆ·æƒ…ç»ªè½¨è¿¹èšç±»...")
        
        # åŠ è½½æ•°æ®
        df_ts = pd.read_parquet('data_multi_users_timeseries.parquet')
        
        # æ„å»ºç”¨æˆ·ç‰¹å¾çŸ©é˜µï¼ˆè½»é‡çº§ï¼‰
        user_features = []
        user_ids = []
        
        # é‡‡æ ·1000ä¸ªç”¨æˆ·é¿å…å†…å­˜é—®é¢˜
        sample_users = df_ts['user_id'].unique()[:48433]
        
        for user_id in sample_users:
            user_data = df_ts[df_ts['user_id'] == user_id]
            if len(user_data) >= 2:
                # æå–ç”¨æˆ·ç‰¹å¾
                features = [
                    user_data['sentiment'].mean(),           # å¹³å‡æƒ…ç»ª
                    user_data['sentiment'].std(),            # æƒ…ç»ªæ³¢åŠ¨
                    user_data['rating'].mean(),              # å¹³å‡è¯„åˆ†
                    len(user_data),                          # æ´»è·ƒåº¦
                    user_data['sentiment_change'].dropna().mean() if len(user_data['sentiment_change'].dropna()) > 0 else 0  # å¹³å‡å˜åŒ–
                ]
                
                user_features.append(features)
                user_ids.append(user_id)
        
        print(f"   æ„å»ºç”¨æˆ·ç‰¹å¾: {len(user_features)} ä¸ªç”¨æˆ·")
        
        # æ‰‹å†™ç®€å•èšç±»ï¼ˆK-meansçš„ç®€åŒ–ç‰ˆï¼‰
        def simple_clustering(features, k=3):
            """ç®€åŒ–çš„èšç±»ç®—æ³•"""
            import random
            random.seed(42)
            
            if len(features) < k:
                return [0] * len(features)
            
            # åˆå§‹åŒ–ä¸­å¿ƒç‚¹
            centers = random.sample(features, k)
            
            for iteration in range(10):  # 10æ¬¡è¿­ä»£
                clusters = []
                
                # åˆ†é…ç‚¹åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
                for point in features:
                    distances = []
                    for center in centers:
                        dist = sum((point[i] - center[i]) ** 2 for i in range(len(point)))
                        distances.append(dist)
                    clusters.append(distances.index(min(distances)))
                
                # æ›´æ–°ä¸­å¿ƒç‚¹
                new_centers = []
                for cluster_id in range(k):
                    cluster_points = [features[i] for i in range(len(features)) if clusters[i] == cluster_id]
                    if cluster_points:
                        new_center = [sum(dim) / len(cluster_points) for dim in zip(*cluster_points)]
                        new_centers.append(new_center)
                    else:
                        new_centers.append(centers[cluster_id])
                
                centers = new_centers
            
            return clusters
        
        # æ‰§è¡Œèšç±»
        user_clusters = simple_clustering(user_features, k=3)
        
        # åˆ†æèšç±»ç»“æœ
        print("2. èšç±»ç»“æœåˆ†æ...")
        cluster_stats = {}
        
        for cluster_id in range(3):
            cluster_features = [user_features[i] for i in range(len(user_features)) if user_clusters[i] == cluster_id]
            if cluster_features:
                cluster_stats[cluster_id] = {
                    'count': len(cluster_features),
                    'avg_sentiment': sum(f[0] for f in cluster_features) / len(cluster_features),
                    'avg_volatility': sum(f[1] for f in cluster_features) / len(cluster_features),
                    'avg_rating': sum(f[2] for f in cluster_features) / len(cluster_features),
                    'avg_activity': sum(f[3] for f in cluster_features) / len(cluster_features)
                }
        
        print("   èšç±»ç‰¹å¾:")
        for cluster_id, stats in cluster_stats.items():
            print(f"     ç¾¤ä½“{cluster_id}: {stats['count']}ç”¨æˆ·")
            print(f"       æƒ…ç»ª: {stats['avg_sentiment']:.3f}, æ³¢åŠ¨: {stats['avg_volatility']:.3f}")
            print(f"       è¯„åˆ†: {stats['avg_rating']:.3f}, æ´»è·ƒåº¦: {stats['avg_activity']:.1f}")
        
        # é˜¶æ®µ2ï¼šç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆç®€åŒ–ç‰ˆSHAPï¼‰
        print("\n3. ç‰¹å¾é‡è¦æ€§åˆ†æ...")
        
        # åŸºäºå›å½’ç³»æ•°ä¼°è®¡ç‰¹å¾é‡è¦æ€§
        importance_analysis = {
            'sentiment_to_rating': 1.716,           # ä»ä¹‹å‰æ¨¡å‹
            'sentiment_change_to_rating_change': 1.572,
            'volatility_impact': 0.018,            # ä»ç”¨æˆ·åˆ†æ
            'activity_impact': 0.195               # é«˜æ´»è·ƒvsä½æ´»è·ƒå·®å¼‚
        }
        
        total_importance = sum(abs(v) for v in importance_analysis.values())
        normalized_importance = {k: abs(v)/total_importance for k, v in importance_analysis.items()}
        
        print("   ç‰¹å¾é‡è¦æ€§æ’åº:")
        sorted_features = sorted(normalized_importance.items(), key=lambda x: x[1], reverse=True)
        for feature, importance in sorted_features:
            print(f"     {feature}: {importance:.3f} ({importance*100:.1f}%)")
        
        # é˜¶æ®µ3ï¼šå¯è§†åŒ–æ•°æ®å‡†å¤‡
        print("\n4. å¯è§†åŒ–æ•°æ®å‡†å¤‡...")
        
        # å‡†å¤‡å›¾è¡¨æ•°æ®
        viz_data = {
            'correlation_summary': {
                'cross_sectional': 0.61,
                'time_series': 0.54,
                'sentiment_change': 0.54,
                'model_r2': 0.34
            },
            'user_segments': cluster_stats,
            'feature_importance': dict(sorted_features),
            'sample_trajectory': []  # ç¤ºä¾‹è½¨è¿¹
        }
        
        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§ç”¨æˆ·è½¨è¿¹
        for cluster_id in range(min(3, len(cluster_stats))):
            cluster_users = [user_ids[i] for i in range(len(user_ids)) if user_clusters[i] == cluster_id]
            if cluster_users:
                sample_user = cluster_users[0]
                user_trajectory = df_ts[df_ts['user_id'] == sample_user][['sentiment', 'rating', 'review_sequence']].to_dict('records')
                viz_data['sample_trajectory'].append({
                    'cluster': cluster_id,
                    'trajectory': user_trajectory[:10]  # å‰10ä¸ªç‚¹
                })
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        final_results = {
            'clustering_results': {
                'user_clusters': dict(zip(user_ids, user_clusters)),
                'cluster_statistics': cluster_stats
            },
            'feature_importance': dict(sorted_features),
            'visualization_data': viz_data,
            'model_summary': {
                'cross_sectional_correlation': 0.61,
                'timeseries_correlation': 0.54,
                'regression_r2': 0.34,
                'effect_size': 'Large'
            }
        }
        
        # ä¿å­˜
        import json
        with open('final_sprint_results.json', 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        print(f"\nâœ… æœ€ç»ˆå†²åˆºåˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š å‘ç°3ä¸ªç”¨æˆ·ç¾¤ä½“ï¼Œå‡†å¤‡å¯è§†åŒ–æ•°æ®")
        print(f"ğŸ¯ é¡¹ç›®è¿›åº¦: 85% â†’ 95%")
        
        return final_results
        
    except Exception as e:
        print(f"âŒ æœ€ç»ˆåˆ†æå¤±è´¥: {e}")
        return None

# æ‰§è¡Œæœ€ç»ˆå†²åˆº
final_results = final_sprint_analysis()
