# === æ—¶åºåˆ†æï¼šå¤šè¯„è®ºç”¨æˆ·æƒ…ç»ªå˜åŒ–æ¨¡å¼ ===
import pandas as pd
import json
import math

def time_series_analysis():
    """
    å¤šè¯„è®ºç”¨æˆ·æ—¶åºåˆ†æ
    æ ¸å¿ƒï¼šåˆ†ææƒ…ç»ªå˜åŒ–å¯¹è¯„åˆ†å’Œç”¨æˆ·è¡Œä¸ºçš„å½±å“
    """
    
    print("=== æ—¶åºåˆ†æå¼€å§‹ ===")
    
    try:
        # 1. åŠ è½½æ—¶åºæ•°æ®
        print("1. åŠ è½½æ—¶åºæ•°æ®...")
        df_ts = pd.read_parquet('data_multi_users_timeseries.parquet')
        print(f"   æ—¶åºæ•°æ®: {len(df_ts):,} æ¡è®°å½•")
        print(f"   ç”¨æˆ·æ•°: {df_ts['user_id'].nunique():,}")
        print(f"   åˆ—å: {list(df_ts.columns)}")
        
        # 2. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n2. æ•°æ®è´¨é‡æ£€æŸ¥...")
        # æ£€æŸ¥å…³é”®åˆ—
        key_cols = ['user_id', 'sentiment', 'rating', 'sentiment_change', 'rating_change']
        missing_info = {}
        for col in key_cols:
            if col in df_ts.columns:
                missing = df_ts[col].isnull().sum()
                missing_info[col] = missing
                print(f"   {col}: {missing:,} ç¼ºå¤± ({missing/len(df_ts)*100:.1f}%)")
        
        # 3. é‡‡æ ·åˆ†æï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        print("\n3. é‡‡æ ·ç”¨æˆ·è¿›è¡Œåˆ†æ...")
        sample_users = df_ts['user_id'].unique()[:1000]  # åˆ†æ1000ä¸ªç”¨æˆ·
        df_sample = df_ts[df_ts['user_id'].isin(sample_users)].copy()
        
        print(f"   é‡‡æ ·ç”¨æˆ·: {len(sample_users):,}")
        print(f"   é‡‡æ ·è®°å½•: {len(df_sample):,}")
        
        # 4. æ—¶åºç‰¹å¾åˆ†æ
        print("\n4. æ—¶åºç‰¹å¾åˆ†æ...")
        
        # 4.1 æƒ…ç»ªå˜åŒ–åˆ†æ
        sentiment_changes = df_sample['sentiment_change'].dropna()
        positive_changes = (sentiment_changes > 0).sum()
        negative_changes = (sentiment_changes < 0).sum()
        no_changes = (sentiment_changes == 0).sum()
        
        print(f"   æƒ…ç»ªå˜åŒ–æ¨¡å¼:")
        print(f"     ä¸Šå‡: {positive_changes:,} ({positive_changes/len(sentiment_changes)*100:.1f}%)")
        print(f"     ä¸‹é™: {negative_changes:,} ({negative_changes/len(sentiment_changes)*100:.1f}%)")
        print(f"     ä¸å˜: {no_changes:,} ({no_changes/len(sentiment_changes)*100:.1f}%)")
        
        # 4.2 è¯„åˆ†å˜åŒ–åˆ†æ
        rating_changes = df_sample['rating_change'].dropna()
        rating_up = (rating_changes > 0).sum()
        rating_down = (rating_changes < 0).sum()
        rating_same = (rating_changes == 0).sum()
        
        print(f"   è¯„åˆ†å˜åŒ–æ¨¡å¼:")
        print(f"     ä¸Šå‡: {rating_up:,} ({rating_up/len(rating_changes)*100:.1f}%)")
        print(f"     ä¸‹é™: {rating_down:,} ({rating_down/len(rating_changes)*100:.1f}%)")
        print(f"     ä¸å˜: {rating_same:,} ({rating_same/len(rating_changes)*100:.1f}%)")
        
        # 5. å‡è®¾éªŒè¯
        print("\n5. å‡è®¾éªŒè¯...")
        
        # H1: æƒ…ç»ªä¸Šå‡ â†’ è¯„åˆ†æå‡
        print("   H1: æƒ…ç»ªä¸Šå‡ â†’ è¯„åˆ†æå‡")
        h1_data = df_sample[['sentiment_change', 'rating_change']].dropna()
        
        if len(h1_data) > 0:
            # æ‰‹å†™ç›¸å…³æ€§è®¡ç®—
            sent_change_list = h1_data['sentiment_change'].tolist()
            rating_change_list = h1_data['rating_change'].tolist()
            
            h1_correlation = manual_correlation(sent_change_list, rating_change_list)
            print(f"     æƒ…ç»ªå˜åŒ–-è¯„åˆ†å˜åŒ–ç›¸å…³æ€§: {h1_correlation:.4f}")
            
            # æ–¹å‘æ€§åˆ†æ
            both_up = ((h1_data['sentiment_change'] > 0) & (h1_data['rating_change'] > 0)).sum()
            both_down = ((h1_data['sentiment_change'] < 0) & (h1_data['rating_change'] < 0)).sum()
            same_direction = both_up + both_down
            
            print(f"     åŒå‘å˜åŒ–æ¯”ä¾‹: {same_direction/len(h1_data)*100:.1f}%")
            print(f"     å…¶ä¸­ä¸Šå‡: {both_up}, ä¸‹é™: {both_down}")
        
        # 6. ç”¨æˆ·ç±»å‹åˆ†æ
        print("\n6. ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ...")
        
        # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„æƒ…ç»ªæ³¢åŠ¨æ€§
        user_volatility = []
        user_avg_sentiment = []
        user_review_counts = []
        
        for user_id in sample_users:
            user_data = df_sample[df_sample['user_id'] == user_id]
            if len(user_data) > 1:
                sentiments = user_data['sentiment'].dropna().tolist()
                if len(sentiments) > 1:
                    # è®¡ç®—æ ‡å‡†å·®ï¼ˆæ³¢åŠ¨æ€§ï¼‰
                    avg_sent = sum(sentiments) / len(sentiments)
                    variance = sum((s - avg_sent) ** 2 for s in sentiments) / len(sentiments)
                    volatility = math.sqrt(variance)
                    
                    user_volatility.append(volatility)
                    user_avg_sentiment.append(avg_sent)
                    user_review_counts.append(len(user_data))
        
        print(f"   åˆ†æç”¨æˆ·æ•°: {len(user_volatility)}")
        if user_volatility:
            avg_volatility = sum(user_volatility) / len(user_volatility)
            avg_sentiment = sum(user_avg_sentiment) / len(user_avg_sentiment)
            avg_reviews = sum(user_review_counts) / len(user_review_counts)
            
            print(f"   å¹³å‡æƒ…ç»ªæ³¢åŠ¨æ€§: {avg_volatility:.3f}")
            print(f"   å¹³å‡æƒ…ç»ªæ°´å¹³: {avg_sentiment:.3f}")
            print(f"   å¹³å‡è¯„è®ºæ•°: {avg_reviews:.1f}")
        
        # 7. ç”¨æˆ·ç•™å­˜åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        print("\n7. ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ...")
        
        # æŒ‰è¯„è®ºæ—¶é—´è·¨åº¦åˆ†ç±»ç”¨æˆ·
        user_spans = []
        for user_id in sample_users:
            user_data = df_sample[df_sample['user_id'] == user_id]
            if len(user_data) > 1 and 'timestamp' in user_data.columns:
                # å¦‚æœæœ‰æ—¶é—´æˆ³ï¼Œè®¡ç®—æ—¶é—´è·¨åº¦
                # è¿™é‡Œç®€åŒ–ä¸ºè¯„è®ºæ•°é‡åˆ†æ
                user_spans.append(len(user_data))
        
        if user_spans:
            # é«˜æ´»è·ƒ vs ä½æ´»è·ƒç”¨æˆ·
            high_active = [s for s in user_spans if s >= 5]  # 5æ¡ä»¥ä¸Š
            low_active = [s for s in user_spans if s < 5]    # 5æ¡ä»¥ä¸‹
            
            print(f"   é«˜æ´»è·ƒç”¨æˆ·: {len(high_active)} ({len(high_active)/len(user_spans)*100:.1f}%)")
            print(f"   ä½æ´»è·ƒç”¨æˆ·: {len(low_active)} ({len(low_active)/len(user_spans)*100:.1f}%)")
        
        # 8. ç»“æœæ±‡æ€»
        results = {
            'sample_stats': {
                'users_analyzed': len(sample_users),
                'records_analyzed': len(df_sample),
                'sentiment_changes_analyzed': len(sentiment_changes)
            },
            'sentiment_change_patterns': {
                'positive_changes_pct': positive_changes/len(sentiment_changes)*100 if len(sentiment_changes) > 0 else 0,
                'negative_changes_pct': negative_changes/len(sentiment_changes)*100 if len(sentiment_changes) > 0 else 0
            },
            'hypothesis_results': {
                'h1_correlation': h1_correlation if 'h1_correlation' in locals() else None,
                'same_direction_pct': same_direction/len(h1_data)*100 if 'h1_data' in locals() and len(h1_data) > 0 else None
            },
            'user_behavior': {
                'avg_volatility': avg_volatility if 'avg_volatility' in locals() else None,
                'avg_sentiment': avg_sentiment if 'avg_sentiment' in locals() else None,
                'high_active_pct': len(high_active)/len(user_spans)*100 if 'high_active' in locals() and user_spans else None
            }
        }
        
        # ä¿å­˜ç»“æœ
        with open('timeseries_analysis_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ… æ—¶åºåˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ° timeseries_analysis_results.json")
        print(f"ğŸ¯ é¡¹ç›®è¿›åº¦: 50% â†’ 75%")
        
        return results
        
    except Exception as e:
        print(f"âŒ æ—¶åºåˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

# æ‰§è¡Œæ—¶åºåˆ†æ
print("ğŸš€ å¼€å§‹é¡¹ç›®æ ¸å¿ƒéƒ¨åˆ†ï¼šæ—¶åºåˆ†æ")
timeseries_results = time_series_analysis()
